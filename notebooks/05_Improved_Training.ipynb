{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "improved_training_title"
   },
   "source": [
    "# IMPROVED HIEROGLYPH DETECTION TRAINING\n",
    "\n",
    "## Based on Detailed Failure Analysis\n",
    "\n",
    "### **Key Findings from Model Analysis:**\n",
    "- **CRITICAL ISSUE**: Model is missing 90%+ of hieroglyphs in test images\n",
    "- **60+ classes** completely missed despite having good training data\n",
    "- **Primary Problem**: Detection threshold too high (0.5 → 0.3)\n",
    "- **Secondary Issues**: Class imbalance, need better augmentation\n",
    "\n",
    "### **Classes That Need Immediate Attention:**\n",
    "- **M17**: 46 in test/val, 452 in training - **MISSED COMPLETELY**\n",
    "- **A1**: 28 in test/val, 209 in training - **MISSED COMPLETELY**\n",
    "- **V1**: 25 in test/val, 252 in training - **MISSED COMPLETELY**\n",
    "- **X1**: 19 in test/val, 165 in training - **MISSED COMPLETELY**\n",
    "- **N35**: 37 in test/val, only 3 detected - **8% detection rate**\n",
    "\n",
    "###  **Training Improvements:**\n",
    "1. **Lower confidence threshold** (0.5 → 0.3)\n",
    "2. **Focal Loss** for hard examples\n",
    "3. **Class weighting** for imbalanced classes\n",
    "4. **Enhanced augmentation** strategies\n",
    "5. **Longer training** with better scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36339,
     "status": "ok",
     "timestamp": 1755892040092,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "setup_colab",
    "outputId": "ecf8ecdb-7f52-4525-933f-12175d6d8bbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Improved Hieroglyph Detection Training...\n",
      "Environment: Google Colab\n",
      "Mounted at /content/drive\n",
      "Current directory: /content/drive/MyDrive/ALP_project\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "GPU memory: 39.6 GB\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Google Colab Setup\n",
    "print('Setting up Improved Hieroglyph Detection Training...')\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f'Environment: {\"Google Colab\"if IN_COLAB else \"Local\"}')\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import os\n",
    "    os.chdir('/content/drive/MyDrive/ALP_project')\n",
    "    print(f'Current directory: {os.getcwd()}')\n",
    "\n",
    "import torch\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n",
    "else:\n",
    "    print('No GPU available - training will be slow!')\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 253067,
     "status": "ok",
     "timestamp": 1755892297799,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "install_detectron2",
    "outputId": "6313e4e3-7e5c-4550-d0e8-e7dc5095c3b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.8.0+cu126\n",
      "Torchvision: 0.23.0+cu126\n",
      "CUDA available: True\n",
      " Installing detectron2...\n",
      "CUDA version: 12.6\n",
      "Installing detectron2 with: pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
      " Detectron2 version: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Install Detectron2 and dependencies for Google Colab\n",
    "import torch\n",
    "import torchvision\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Torchvision: {torchvision.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Install detectron2\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_detectron2():\n",
    "    \"\"\"Install detectron2 based on PyTorch version\"\"\"\n",
    "    torch_version = torch.__version__\n",
    "    torchvision_version = torchvision.__version__\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        cuda_version = torch.version.cuda\n",
    "        print(f\"CUDA version: {cuda_version}\")\n",
    "\n",
    "        # Install for GPU\n",
    "        if cuda_version.startswith('11') or cuda_version.startswith('12'):\n",
    "            cmd = \"pip install 'git+https://github.com/facebookresearch/detectron2.git'\"\n",
    "        else:\n",
    "            cmd = \"pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/index.html\"\n",
    "    else:\n",
    "        # Install for CPU\n",
    "        cmd = \"pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch{}/index.html\".format(torch_version[:3])\n",
    "\n",
    "    print(f\"Installing detectron2 with: {cmd}\")\n",
    "    subprocess.check_call(cmd, shell=True)\n",
    "\n",
    "try:\n",
    "    import detectron2\n",
    "    print(\"Detectron2 already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing detectron2...\")\n",
    "    install_detectron2()\n",
    "    import detectron2\n",
    "\n",
    "print(f\"Detectron2 version: {detectron2.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 914,
     "status": "ok",
     "timestamp": 1755892298717,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "import_libraries",
    "outputId": "a5d2d064-8a51-456e-bf3c-b710a094c805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Detectron2 imports\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.solver import build_lr_scheduler, build_optimizer\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers\n",
    "from detectron2.utils.events import EventStorage\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "focal_loss_implementation"
   },
   "source": [
    "## Focal Loss Implementation\n",
    "\n",
    "Focal Loss helps with hard examples and class imbalance - exactly what we need for the missed hieroglyphs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1755892494054,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "implement_focal_loss",
    "outputId": "4190c14c-d80f-4cbd-80c1-2276fed3ef54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal Loss implementation ready!\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for addressing class imbalance and hard examples\"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Ensure targets are on the same device as inputs\n",
    "        targets = targets.to(inputs.device)\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class ImprovedFastRCNNOutputLayers(FastRCNNOutputLayers):\n",
    "    \"\"\"Custom output layer with Focal Loss\"\"\"\n",
    "\n",
    "    def __init__(self, cfg, input_shape):\n",
    "        super().__init__(cfg, input_shape)\n",
    "        self.focal_loss = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "        self.use_focal_loss = True\n",
    "\n",
    "    def losses(self, predictions, proposals):\n",
    "        scores, proposal_deltas = predictions\n",
    "        gt_classes = (\n",
    "            torch.cat([p.gt_classes for p in proposals], dim=0) if len(proposals) else torch.empty(0, dtype=torch.long, device=scores.device)\n",
    "        )\n",
    "\n",
    "        # Ensure gt_classes are on the same device as scores for Focal Loss calculation\n",
    "        gt_classes_gpu = gt_classes.to(scores.device)\n",
    "\n",
    "        if self.use_focal_loss and len(gt_classes_gpu) > 0:\n",
    "            # Use Focal Loss instead of standard cross entropy\n",
    "            loss_cls = self.focal_loss(scores, gt_classes_gpu)\n",
    "        else:\n",
    "            # Fallback to standard loss (using GPU tensors)\n",
    "             loss_cls = F.cross_entropy(scores, gt_classes_gpu, reduction=\"mean\")\n",
    "\n",
    "\n",
    "        # Box regression loss and other base class logic\n",
    "        # Move necessary tensors to CPU before calling super().losses() for internal logging/stats\n",
    "        proposals_cpu = []\n",
    "        for p in proposals:\n",
    "            p_cpu = p.to(\"cpu\") # Move the entire Instance object to CPU\n",
    "            proposals_cpu.append(p_cpu)\n",
    "\n",
    "        # Call the base class's losses method with CPU tensors\n",
    "        losses = super().losses((scores.to(\"cpu\"), proposal_deltas.to(\"cpu\")), proposals_cpu)\n",
    "\n",
    "        # Replace the base class's classification loss with our Focal Loss\n",
    "        losses[\"loss_cls\"] = loss_cls\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the improved output layers.\n",
    "        Ensures input tensor is on the correct device.\n",
    "        \"\"\"\n",
    "        # Ensure the input tensor is on the same device as the layer's parameters\n",
    "        x = x.to(self.cls_score.weight.device)\n",
    "\n",
    "        if x.dim() > 2:\n",
    "            x = torch.flatten(x, start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        proposal_deltas = self.bbox_pred(x)\n",
    "        return scores, proposal_deltas\n",
    "\n",
    "\n",
    "print(\"Focal Loss implementation ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loading_section"
   },
   "source": [
    "## Enhanced Data Loading\n",
    "\n",
    "Loading data with class weights and priority focus on missed classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1755892389247,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "load_and_analyze_data",
    "outputId": "a5c01b1a-3acf-49f1-dafb-98feef0e07aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Dataset: 42 images, 4726 annotations\n",
      "Classes: 634 total\n",
      "Class distribution (top 10):\n",
      "   M17: 452\n",
      "   N35: 380\n",
      "   V1: 252\n",
      "   A1: 209\n",
      "   X1: 165\n",
      "   G7: 156\n",
      "   I9: 156\n",
      "   R11: 154\n",
      "   S29: 130\n",
      "   Z1: 126\n",
      "\n",
      "Loading validation data...\n",
      "Dataset: 2 images, 275 annotations\n",
      "Classes: 634 total\n",
      "Class distribution (top 10):\n",
      "   M17: 30\n",
      "   N35: 25\n",
      "   V1: 18\n",
      "   A1: 17\n",
      "   R11: 12\n",
      "   X1: 10\n",
      "   Y1: 10\n",
      "   I9: 9\n",
      "   A2: 9\n",
      "   I10: 8\n",
      "\n",
      "Loading test data...\n",
      "Dataset: 1 images, 191 annotations\n",
      "Classes: 634 total\n",
      "Class distribution (top 10):\n",
      "   M17: 16\n",
      "   N35: 12\n",
      "   A1: 11\n",
      "   G1: 10\n",
      "   X1: 9\n",
      "   V1: 7\n",
      "   D21: 7\n",
      "   Y1: 7\n",
      "   S29: 6\n",
      "   D36: 6\n",
      "42/42 images contain critical classes\n",
      "2/2 images contain critical classes\n",
      "1/1 images contain critical classes\n",
      "\n",
      "Data loading complete!\n",
      "Train: 42 images\n",
      "Val: 2 images\n",
      "Test: 1 images\n"
     ]
    }
   ],
   "source": [
    "def load_hieroglyph_data(json_file):\n",
    "    \"\"\"Load hieroglyph dataset with enhanced analysis\"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Create mappings\n",
    "    images = {img['id']: img for img in data['images']}\n",
    "    categories = {cat['id']: cat for cat in data['categories']}\n",
    "\n",
    "    # Analyze class distribution\n",
    "    class_counts = {}\n",
    "    for ann in data['annotations']:\n",
    "        cat_name = categories[ann['category_id']]['name']\n",
    "        class_counts[cat_name] = class_counts.get(cat_name, 0) + 1\n",
    "\n",
    "    print(f\"Dataset: {len(data['images'])} images, {len(data['annotations'])} annotations\")\n",
    "    print(f\"Classes: {len(categories)} total\")\n",
    "    print(f\"Class distribution (top 10):\")\n",
    "    for cls, count in sorted(class_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"{cls}: {count}\")\n",
    "\n",
    "    return data, images, categories, class_counts\n",
    "\n",
    "def convert_to_detectron_format(data, images, categories, split_name):\n",
    "    \"\"\"Convert to Detectron2 format with class weights\"\"\"\n",
    "    dataset_dicts = []\n",
    "\n",
    "    # Critical classes that were completely missed\n",
    "    critical_classes = {\n",
    "        'M17', 'A1', 'V1', 'X1', 'Y1', 'D21', 'G1', 'S29',\n",
    "        'Aa1', 'D36', 'Z1', 'Z4', 'U33', 'V31', 'G17'\n",
    "    }\n",
    "\n",
    "    # Group annotations by image\n",
    "    image_annotations = {}\n",
    "    for ann in data['annotations']:\n",
    "        img_id = ann['image_id']\n",
    "        if img_id not in image_annotations:\n",
    "            image_annotations[img_id] = []\n",
    "        image_annotations[img_id].append(ann)\n",
    "\n",
    "    for img_id, img_info in images.items():\n",
    "        if img_id not in image_annotations:\n",
    "            continue\n",
    "\n",
    "        record = {\n",
    "            \"file_name\": os.path.join(f\"hieroglyphs_dataset/{split_name}/images\", img_info[\"file_name\"]),\n",
    "            \"image_id\": img_id,\n",
    "            \"height\": img_info[\"height\"],\n",
    "            \"width\": img_info[\"width\"]\n",
    "        }\n",
    "\n",
    "        annotations = []\n",
    "        has_critical_class = False\n",
    "\n",
    "        for ann in image_annotations[img_id]:\n",
    "            cat_name = categories[ann['category_id']]['name']\n",
    "\n",
    "            # Check if this image has critical classes\n",
    "            if cat_name in critical_classes:\n",
    "                has_critical_class = True\n",
    "\n",
    "            bbox = ann[\"bbox\"]\n",
    "            annotations.append({\n",
    "                \"bbox\": bbox,\n",
    "                \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                \"segmentation\": [],\n",
    "                \"category_id\": ann[\"category_id\"] - 1,  # Detectron2 uses 0-based indexing\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "\n",
    "        record[\"annotations\"] = annotations\n",
    "        record[\"has_critical_class\"] = has_critical_class\n",
    "        dataset_dicts.append(record)\n",
    "\n",
    "    critical_images = sum(1 for r in dataset_dicts if r[\"has_critical_class\"])\n",
    "    print(f\"{critical_images}/{len(dataset_dicts)} images contain critical classes\")\n",
    "\n",
    "    return dataset_dicts\n",
    "\n",
    "# Load all datasets\n",
    "print(\"Loading training data...\")\n",
    "train_data, train_images, train_categories, train_class_counts = load_hieroglyph_data(\n",
    "    \"hieroglyphs_dataset/train_augmented/annotations.json\"\n",
    ")\n",
    "\n",
    "print(\"\\nLoading validation data...\")\n",
    "val_data, val_images, val_categories, val_class_counts = load_hieroglyph_data(\n",
    "    \"hieroglyphs_dataset/val/annotations.json\"\n",
    ")\n",
    "\n",
    "print(\"\\nLoading test data...\")\n",
    "test_data, test_images, test_categories, test_class_counts = load_hieroglyph_data(\n",
    "    \"hieroglyphs_dataset/test/annotations.json\"\n",
    ")\n",
    "\n",
    "# Convert to Detectron2 format\n",
    "train_dataset = convert_to_detectron_format(train_data, train_images, train_categories, \"train_augmented\")\n",
    "val_dataset = convert_to_detectron_format(val_data, val_images, val_categories, \"val\")\n",
    "test_dataset = convert_to_detectron_format(test_data, test_images, test_categories, \"test\")\n",
    "\n",
    "print(f\"\\nData loading complete!\")\n",
    "print(f\"Train: {len(train_dataset)} images\")\n",
    "print(f\"Val: {len(val_dataset)} images\")\n",
    "print(f\"Test: {len(test_dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1755892395039,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "register_datasets",
    "outputId": "56bfd20d-82f5-4dde-9077-83f8f9ecd45b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets registered with 634 classes\n",
      " Sample classes: ['A1', 'A121C', 'A13', 'A131A', 'A13A', 'A14', 'A15', 'A16', 'A169', 'A17']...\n"
     ]
    }
   ],
   "source": [
    "# Register datasets with Detectron2\n",
    "def get_hieroglyph_train():\n",
    "    return train_dataset\n",
    "\n",
    "def get_hieroglyph_val():\n",
    "    return val_dataset\n",
    "\n",
    "def get_hieroglyph_test():\n",
    "    return test_dataset\n",
    "\n",
    "# Clear existing registrations\n",
    "for dataset_name in [\"hieroglyph_train_improved\", \"hieroglyph_val_improved\", \"hieroglyph_test_improved\"]:\n",
    "    if dataset_name in DatasetCatalog:\n",
    "        DatasetCatalog.remove(dataset_name)\n",
    "        MetadataCatalog.remove(dataset_name)\n",
    "\n",
    "# Register datasets\n",
    "DatasetCatalog.register(\"hieroglyph_train_improved\", get_hieroglyph_train)\n",
    "DatasetCatalog.register(\"hieroglyph_val_improved\", get_hieroglyph_val)\n",
    "DatasetCatalog.register(\"hieroglyph_test_improved\", get_hieroglyph_test)\n",
    "\n",
    "# Set metadata\n",
    "class_names = [cat['name'] for cat in sorted(train_categories.values(), key=lambda x: x['id'])]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "for dataset_name in [\"hieroglyph_train_improved\", \"hieroglyph_val_improved\", \"hieroglyph_test_improved\"]:\n",
    "    MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
    "    MetadataCatalog.get(dataset_name).num_classes = num_classes\n",
    "\n",
    "print(f\"Datasets registered with {num_classes} classes\")\n",
    "print(f\"Sample classes: {class_names[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "improved_trainer_section"
   },
   "source": [
    "## Improved Trainer with Focal Loss\n",
    "\n",
    "Custom trainer that addresses the specific issues found in the failure analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1755893743046,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "implement_improved_trainer",
    "outputId": "a38e5b4f-6b5b-4a43-feaf-2a63da97d8b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved trainer ready!\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import torch.distributed as dist\n",
    "from detectron2.utils import comm\n",
    "from detectron2.structures import Instances, Boxes\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset, print_csv_format\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "class ImprovedHieroglyphTrainer(DefaultTrainer):\n",
    "    \"\"\"Enhanced trainer for hieroglyph detection with failure analysis improvements\"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "        self.best_map = 0.0\n",
    "        self.patience = 0\n",
    "        self.max_patience = 20\n",
    "\n",
    "    @classmethod\n",
    "    def build_model(cls, cfg):\n",
    "        \"\"\"Build model with Focal Loss\"\"\"\n",
    "        model = build_model(cfg)\n",
    "\n",
    "        # Replace the classifier head with our improved version\n",
    "        if hasattr(model.roi_heads, 'box_head'):\n",
    "            # Get the input shape for the box predictor\n",
    "            input_shape = model.roi_heads.box_head.output_shape\n",
    "\n",
    "            # Replace with our improved predictor\n",
    "            model.roi_heads.box_predictor = ImprovedFastRCNNOutputLayers(cfg, input_shape)\n",
    "\n",
    "        print(\"Model built with Focal Loss!\")\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name):\n",
    "        \"\"\"Build evaluator for validation\"\"\"\n",
    "        output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return COCOEvaluator(dataset_name, output_dir=output_folder)\n",
    "\n",
    "    # Removed the overridden run_step method to prevent conflicting lr logging.\n",
    "    # The base DefaultTrainer's run_step is now used.\n",
    "\n",
    "    def after_step(self):\n",
    "        \"\"\"Enhanced after step with early stopping\"\"\"\n",
    "        super().after_step()\n",
    "\n",
    "        # Run validation every 500 iterations\n",
    "        if (self.iter + 1) % 500 == 0 and self.iter > 1000:\n",
    "            self.validate_and_save()\n",
    "\n",
    "    def test(self, cfg, model, evaluators=None):\n",
    "        \"\"\"\n",
    "        Evaluate the given model.\n",
    "        Add explicit device placement for inputs during evaluation.\n",
    "        Also, temporarily set model to eval mode for inference.\n",
    "        \"\"\"\n",
    "        # Replicate DefaultTrainer.test logic but with device placement and eval mode\n",
    "        results = OrderedDict()\n",
    "        for idx, dataset_name in enumerate(cfg.DATASETS.TEST):\n",
    "            data_loader = build_detection_test_loader(cfg, dataset_name)\n",
    "            evaluator = self.build_evaluator(cfg, dataset_name) if evaluators is None else evaluators[idx]\n",
    "\n",
    "            print(f\"Evaluating on {dataset_name}...\")\n",
    "            evaluator.reset()\n",
    "\n",
    "            # Temporarily set model to evaluation mode\n",
    "            is_training = model.training\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs in tqdm(data_loader, desc=f\"Evaluating {dataset_name}\"):\n",
    "                    # Explicitly move inputs to the model's device\n",
    "                    inputs = self._move_inputs_to_device(inputs, cfg.MODEL.DEVICE)\n",
    "                    outputs = model(inputs)\n",
    "                    evaluator.process(inputs, outputs)\n",
    "\n",
    "            results_i = evaluator.evaluate()\n",
    "            results[dataset_name] = results_i\n",
    "\n",
    "            # Restore model to original training mode\n",
    "            model.train(is_training)\n",
    "\n",
    "\n",
    "        if comm.is_main_process():\n",
    "            assert isinstance(results, dict), \"Evaluator must return a dict\"\n",
    "            print(f\"Evaluation results: {results}\")\n",
    "            # Optional: print results in csv format\n",
    "            # print_csv_format(results) # Commented out to avoid potential formatting issues\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _move_inputs_to_device(self, inputs, device):\n",
    "        \"\"\"Moves batched inputs to the specified device.\"\"\"\n",
    "        processed_inputs = []\n",
    "        for input_dict in inputs:\n",
    "            processed_dict = {}\n",
    "            for k, v in input_dict.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    processed_dict[k] = v.to(device)\n",
    "                elif isinstance(v, Instances):\n",
    "                    # Move the entire Instances object\n",
    "                    v = v.to(device)\n",
    "                    # Explicitly ensure proposal_boxes tensor is on device if it exists\n",
    "                    if hasattr(v, 'proposal_boxes') and isinstance(v.proposal_boxes, Boxes):\n",
    "                         v.proposal_boxes.tensor = v.proposal_boxes.tensor.to(device)\n",
    "                    processed_dict[k] = v\n",
    "                else:\n",
    "                    processed_dict[k] = v\n",
    "            processed_inputs.append(processed_dict)\n",
    "        return processed_inputs\n",
    "\n",
    "    def validate_and_save(self):\n",
    "        \"\"\"Validation with early stopping\"\"\"\n",
    "        try:\n",
    "            # Run validation using the overridden test method\n",
    "            val_results = self.test(self.cfg, self.model, [self.build_evaluator(self.cfg, \"hieroglyph_val_improved\")]) # Pass evaluator as a list\n",
    "            val_map = val_results[\"hieroglyph_val_improved\"][\"bbox\"][\"AP\"]\n",
    "\n",
    "            print(f\"Validation mAP at iter {self.iter}: {val_map:.3f} (Best: {self.best_map:.3f})\")\n",
    "\n",
    "            # Early stopping logic\n",
    "            if val_map > self.best_map:\n",
    "                self.best_map = val_map\n",
    "                self.patience = 0\n",
    "                # Save best model\n",
    "                self.checkpointer.save(f\"model_best_map_{val_map:.3f}_iter_{self.iter}\")\n",
    "                print(f\"New best model saved! mAP: {val_map:.3f}\")\n",
    "            else:\n",
    "                self.patience += 1\n",
    "                print(f\"⏰ Patience: {self.patience}/{self.max_patience}\")\n",
    "\n",
    "                if self.patience >= self.max_patience:\n",
    "                    print(f\"Early stopping triggered at iteration {self.iter}\")\n",
    "                    # Stop training\n",
    "                    self.max_iter = self.iter\n",
    "\n",
    "            # Log validation metrics\n",
    "            self.storage.put_scalar(\"validation/AP\", val_map)\n",
    "            self.storage.put_scalar(\"validation/best_AP\", self.best_map)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Validation failed: {e}\")\n",
    "\n",
    "print(\"Improved trainer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "configuration_section"
   },
   "source": [
    "## Training Configuration\n",
    "\n",
    "Based on failure analysis - focusing on the specific issues identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1755893025020,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "setup_improved_config",
    "outputId": "4ed2208b-b593-457e-cb50-0b52af062dbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPROVED CONFIGURATION HIGHLIGHTS:\n",
      "   Confidence threshold: 0.3 (was 0.5)\n",
      "   Using Focal Loss for hard examples\n",
      "   Max detections per image: 300\n",
      "   Training iterations: 15000\n",
      "   Base Learning Rate: 0.0005 (was 0.001)\n",
      "    Gradient Clipping Enabled: True\n",
      "   Output directory: ./output/improved_training_20250822_200344\n",
      "Configuration ready for improved training!\n"
     ]
    }
   ],
   "source": [
    "def setup_improved_config():\n",
    "    \"\"\"Setup improved training configuration based on failure analysis\"\"\"\n",
    "    cfg = get_cfg()\n",
    "\n",
    "    # Base model\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "\n",
    "    # Dataset configuration\n",
    "    cfg.DATASETS.TRAIN = (\"hieroglyph_train_improved\",)\n",
    "    cfg.DATASETS.TEST = (\"hieroglyph_val_improved\",)\n",
    "    cfg.DATALOADER.NUM_WORKERS = 4\n",
    "\n",
    "    # Model configuration\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes\n",
    "\n",
    "    # CRITICAL FIX: Lower confidence threshold from 0.5 to 0.3\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3  # This was the main issue!\n",
    "    cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.4\n",
    "    cfg.TEST.DETECTIONS_PER_IMAGE = 300  # Allow more detections\n",
    "\n",
    "    # Training configuration\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 4 if torch.cuda.is_available() else 1\n",
    "    cfg.SOLVER.BASE_LR = 0.0005  # Slightly lower LR for stability (was 0.001)\n",
    "    cfg.SOLVER.MAX_ITER = 15000  # Longer training\n",
    "    cfg.SOLVER.STEPS = (8000, 12000)  # Learning rate schedule\n",
    "    cfg.SOLVER.GAMMA = 0.1\n",
    "    cfg.SOLVER.WARMUP_ITERS = 500\n",
    "    cfg.SOLVER.WARMUP_FACTOR = 0.001\n",
    "\n",
    "    # Gradient Clipping to prevent divergence\n",
    "    cfg.SOLVER.CLIP_GRADIENTS.ENABLED = True\n",
    "    cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE = \"norm\"\n",
    "    cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE = 1.0 # Clip gradients with norm > 1.0\n",
    "\n",
    "    # Enhanced data augmentation for missed classes\n",
    "    cfg.INPUT.MIN_SIZE_TRAIN = (480, 512, 544, 576, 608, 640)  # Multi-scale training\n",
    "    cfg.INPUT.MAX_SIZE_TRAIN = 1024\n",
    "    cfg.INPUT.MIN_SIZE_TEST = 512\n",
    "    cfg.INPUT.MAX_SIZE_TEST = 1024\n",
    "\n",
    "    # Data augmentation\n",
    "    cfg.INPUT.BRIGHTNESS = 0.2\n",
    "    cfg.INPUT.CONTRAST = 0.2\n",
    "    cfg.INPUT.SATURATION = 0.2\n",
    "    cfg.INPUT.HUE = 0.1\n",
    "\n",
    "    # Output directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    cfg.OUTPUT_DIR = f\"./output/improved_training_{timestamp}\"\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # Evaluation\n",
    "    cfg.TEST.EVAL_PERIOD = 500\n",
    "\n",
    "    # Device\n",
    "    cfg.MODEL.DEVICE = \"cuda\"if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    print(\"IMPROVED CONFIGURATION HIGHLIGHTS:\")\n",
    "    print(f\"Confidence threshold: {cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST} (was 0.5)\")\n",
    "    print(f\"Using Focal Loss for hard examples\")\n",
    "    print(f\"Max detections per image: {cfg.TEST.DETECTIONS_PER_IMAGE}\")\n",
    "    print(f\"Training iterations: {cfg.SOLVER.MAX_ITER}\")\n",
    "    print(f\"Base Learning Rate: {cfg.SOLVER.BASE_LR} (was 0.001)\")\n",
    "    print(f\"Gradient Clipping Enabled: {cfg.SOLVER.CLIP_GRADIENTS.ENABLED}\")\n",
    "    print(f\"Output directory: {cfg.OUTPUT_DIR}\")\n",
    "\n",
    "    return cfg\n",
    "\n",
    "# Setup configuration\n",
    "cfg = setup_improved_config()\n",
    "\n",
    "print(\"Configuration ready for improved training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_section"
   },
   "source": [
    "##  IMPROVED TRAINING\n",
    "\n",
    "Training with all the improvements to address the missed hieroglyphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2312860,
     "status": "ok",
     "timestamp": 1755896300067,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "start_improved_training",
    "outputId": "987c3d8a-b841-4598-adb3-992c06c7dbc5"
   },
   "outputs": [],
   "source": [
    "print(\"STARTING IMPROVED HIEROGLYPH DETECTION TRAINING (FROM SCRATCH)\")\n",
    "print(\"=\"*60)\n",
    "print(\"ADDRESSING CRITICAL ISSUES:\")\n",
    "print(\"Previous model missed 90%+ of hieroglyphs\")\n",
    "print(\"Lowered confidence threshold: 0.5 → 0.3\")\n",
    "print(\"Added Focal Loss for hard examples\")\n",
    "print(\"Enhanced augmentation strategies\")\n",
    "print(\"Longer training with early stopping\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create trainer\n",
    "trainer = ImprovedHieroglyphTrainer(cfg)\n",
    "\n",
    "# Start training from pretrained COCO weights as specified in cfg.MODEL.WEIGHTS\n",
    "# The trainer automatically loads cfg.MODEL.WEIGHTS when train() is called without resuming\n",
    "print(\"Starting training from pretrained COCO weights specified in config.\")\n",
    "\n",
    "# DEVICE FIX - Add this cell before training\n",
    "print(\"Fixing device placement issues...\")\n",
    "\n",
    "# Disable evaluation during training to avoid device conflicts\n",
    "cfg.TEST.EVAL_PERIOD = 0  # This prevents the error\n",
    "\n",
    "# Ensure all model components are on GPU\n",
    "if torch.cuda.is_available():\n",
    "    trainer.model = trainer.model.cuda()\n",
    "    print(\"Model moved to GPU\")\n",
    "\n",
    "print(\"Device fix applied - training should work now!\")\n",
    "\n",
    "# Start training\n",
    "print(f\"\\n Starting training for {cfg.SOLVER.MAX_ITER} iterations...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nIMPROVED TRAINING COMPLETE!\")\n",
    "# Accessing best_map might cause issues if training failed early. Add check.\n",
    "if hasattr(trainer, 'best_map'):\n",
    "    print(f\"Best validation mAP: {trainer.best_map:.3f}\")\n",
    "else:\n",
    "    print(\"Validation mAP not available (training did not reach validation step)\")\n",
    "\n",
    "print(f\"Model saved to: {cfg.OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation_section"
   },
   "source": [
    "## Evaluation with Lower Threshold\n",
    "\n",
    "Test the improved model with the lower confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3065,
     "status": "ok",
     "timestamp": 1755896303141,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "evaluate_improved_model",
    "outputId": "03a8548f-edc6-4e53-d4f8-1415d9a83101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING IMPROVED MODEL\n",
      "========================================\n",
      "Model built with Focal Loss!\n",
      "Loading latest checkpoint: ./output/improved_training_20250822_200344/model_final.pth\n",
      "[08/22 20:58:20 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from ./output/improved_training_20250822_200344/model_final.pth ...\n",
      "Checkpoint loaded.\n",
      "Using confidence threshold: 0.3\n",
      "[08/22 20:58:20 d2.evaluation.coco_evaluation]: Trying to convert 'hieroglyph_test_improved' to COCO format ...\n",
      "[08/22 20:58:20 d2.data.datasets.coco]: Converting annotations of dataset 'hieroglyph_test_improved' to COCO format ...)\n",
      "[08/22 20:58:20 d2.data.datasets.coco]: Converting dataset dicts into COCO format\n",
      "[08/22 20:58:20 d2.data.datasets.coco]: Conversion finished, #images: 1, #annotations: 191\n",
      "[08/22 20:58:21 d2.data.datasets.coco]: Caching COCO format annotations at './output/improved_training_20250822_200344/hieroglyph_test_improved_coco_format.json' ...\n",
      "[08/22 20:58:21 d2.data.build]: Distribution of instances among all 634 categories:\n",
      "|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
      "|     A1     | 11           |   A121C    | 1            |    A13     | 0            |\n",
      "|   A131A    | 0            |    A13A    | 0            |    A14     | 0            |\n",
      "|    A15     | 0            |    A16     | 0            |    A169    | 0            |\n",
      "|    A17     | 0            |    A183    | 0            |    A19     | 0            |\n",
      "|    A1B     | 0            |     A2     | 4            |    A20     | 0            |\n",
      "|    A21     | 0            |    A21A    | 0            |    A22     | 0            |\n",
      "|    A23     | 0            |   A239B    | 0            |    A23A    | 0            |\n",
      "|    A24     | 1            |    A25     | 0            |    A26     | 0            |\n",
      "|    A27     | 0            |    A28     | 0            |    A29     | 0            |\n",
      "|     A3     | 0            |    A30     | 0            |    A317    | 0            |\n",
      "|    A32A    | 0            |    A35     | 0            |     A4     | 0            |\n",
      "|    A40     | 0            |    A406    | 0            |    A42     | 0            |\n",
      "|    A43B    | 0            |    A44     | 0            |    A45A    | 0            |\n",
      "|    A47     | 0            |    A49     | 0            |    A50     | 1            |\n",
      "|    A51     | 0            |    A52     | 0            |    A53     | 0            |\n",
      "|    A55     | 0            |    A73     | 0            |    A7A     | 0            |\n",
      "|     A8     | 0            |    A81     | 0            |    A86     | 0            |\n",
      "|     A9     | 0            |    A90     | 0            |    A9C     | 0            |\n",
      "|     B1     | 1            |    B10     | 0            |    B12     | 0            |\n",
      "|    B17     | 0            |     B2     | 0            |    B2A     | 0            |\n",
      "|     B3     | 0            |    B3A     | 0            |    B3B     | 0            |\n",
      "|     B5     | 0            |    B59     | 0            |    B5C     | 0            |\n",
      "|     C1     | 0            |    C10     | 0            |    C10A    | 0            |\n",
      "|    C11     | 0            |    C164    | 0            |    C19     | 0            |\n",
      "|    C1D     | 0            |     C4     | 0            |     C9     | 0            |\n",
      "|     D1     | 3            |    D10     | 0            |    D12     | 0            |\n",
      "|    D13     | 0            |    D143    | 0            |    D19     | 0            |\n",
      "|    D192    | 0            |    D198    | 0            |     D2     | 1            |\n",
      "|    D20     | 0            |    D200    | 0            |    D21     | 7            |\n",
      "|    D24     | 0            |    D245    | 0            |    D25     | 0            |\n",
      "|    D26     | 0            |    D26A    | 0            |    D28     | 0            |\n",
      "|    D283    | 0            |    D284    | 0            |    D285    | 0            |\n",
      "|     D3     | 0            |    D329    | 0            |    D33     | 0            |\n",
      "|    D33C    | 0            |    D34     | 0            |    D35     | 2            |\n",
      "|    D36     | 6            |    D37     | 0            |    D38     | 0            |\n",
      "|    D39     | 0            |    D3B     | 0            |     D4     | 1            |\n",
      "|    D40     | 1            |    D41     | 0            |    D42     | 0            |\n",
      "|    D43     | 0            |    D45     | 0            |    D46     | 4            |\n",
      "|    D49     | 0            |     D5     | 0            |    D50     | 0            |\n",
      "|    D51     | 0            |    D52     | 0            |    D53     | 0            |\n",
      "|    D54     | 1            |    D55     | 0            |    D56     | 0            |\n",
      "|    D57     | 0            |    D57B    | 0            |    D58     | 2            |\n",
      "|     D6     | 0            |    D60     | 0            |    D63     | 0            |\n",
      "|    D7C     | 0            |    D9A     | 0            |     E1     | 0            |\n",
      "|    E10     | 0            |    E100    | 0            |    E11     | 0            |\n",
      "|    E11B    | 0            |    E12     | 0            |    E128    | 0            |\n",
      "|    E13     | 0            |    E14     | 0            |    E15     | 0            |\n",
      "|    E16     | 0            |    E164    | 0            |    E17     | 0            |\n",
      "|    E18     | 0            |    E186    | 0            |    E1A     | 0            |\n",
      "|     E2     | 0            |    E23     | 0            |    E263    | 0            |\n",
      "|    E27     | 0            |     E3     | 0            |    E31     | 0            |\n",
      "|    E32     | 0            |    E34     | 0            |    E35     | 0            |\n",
      "|    E51     | 0            |     E7     | 0            |     E8     | 1            |\n",
      "|     E9     | 0            |     F1     | 0            |    F10     | 0            |\n",
      "|    F12     | 0            |    F13     | 0            |    F16     | 0            |\n",
      "|    F17     | 0            |    F18     | 0            |    F19     | 0            |\n",
      "|    F20     | 0            |    F21     | 0            |    F22     | 0            |\n",
      "|    F23     | 0            |    F25     | 0            |    F25A    | 0            |\n",
      "|    F26     | 0            |    F27     | 0            |    F29     | 0            |\n",
      "|    F2B     | 0            |    F30     | 0            |    F30A    | 0            |\n",
      "|    F31     | 0            |    F32     | 0            |    F33     | 0            |\n",
      "|    F34     | 0            |    F35     | 0            |    F36     | 0            |\n",
      "|    F37     | 0            |    F37B    | 0            |    F37C    | 0            |\n",
      "|    F37I    | 0            |    F37N    | 0            |    F39     | 0            |\n",
      "|     F4     | 0            |    F40     | 0            |    F41     | 0            |\n",
      "|    F42     | 0            |    F43     | 0            |    F44     | 0            |\n",
      "|    F45     | 0            |    F46     | 0            |    F47     | 0            |\n",
      "|     F5     | 0            |    F50     | 0            |    F51     | 0            |\n",
      "|     F7     | 0            |    F7C     | 0            |     F8     | 0            |\n",
      "|     F9     | 0            |     G1     | 10           |    G14     | 0            |\n",
      "|    G17     | 3            |    G17A    | 0            |    G18     | 0            |\n",
      "|    G197    | 0            |     G2     | 0            |    G21     | 0            |\n",
      "|    G21A    | 0            |    G21B    | 0            |    G21C    | 0            |\n",
      "|    G22     | 0            |    G23     | 0            |    G25     | 0            |\n",
      "|    G253    | 0            |    G26     | 0            |    G26A    | 0            |\n",
      "|    G26B    | 0            |    G27     | 0            |    G28     | 0            |\n",
      "|    G29     | 2            |    G30     | 0            |    G30A    | 0            |\n",
      "|    G31     | 0            |    G32     | 0            |    G35     | 0            |\n",
      "|    G36     | 1            |    G37     | 0            |    G38     | 2            |\n",
      "|    G39     | 0            |     G4     | 0            |    G40     | 0            |\n",
      "|    G41     | 3            |    G43     | 0            |    G47     | 0            |\n",
      "|    G49C    | 0            |    G49E    | 0            |    G4A     | 0            |\n",
      "|     G5     | 0            |    G50     | 0            |    G51     | 0            |\n",
      "|    G54     | 0            |     G6     | 0            |     H1     | 0            |\n",
      "|     H3     | 0            |     H5     | 0            |     H6     | 0            |\n",
      "|     H8     | 0            |     I1     | 0            |    I10     | 4            |\n",
      "|    I12     | 0            |    I13     | 0            |    I14     | 0            |\n",
      "|     I2     | 0            |    I24A    | 0            |     I3     | 0            |\n",
      "|     I5     | 0            |     I6     | 0            |    I70B    | 0            |\n",
      "|    I70D    | 0            |    I71A    | 0            |     I8     | 0            |\n",
      "|    I86C    | 0            |    I86D    | 0            |    I87     | 0            |\n",
      "|     I9     | 3            |    I9A     | 0            |     K1     | 0            |\n",
      "|     K2     | 0            |     K3     | 0            |     K4     | 0            |\n",
      "|     K5     | 0            |     K7     | 0            |     L1     | 0            |\n",
      "|     L2     | 0            |     L4     | 0            |     L5     | 0            |\n",
      "|     L7     | 0            |     M1     | 0            |    M11     | 0            |\n",
      "|    M12     | 0            |    M13     | 0            |    M14     | 0            |\n",
      "|    M15     | 2            |    M16     | 0            |    M17     | 16           |\n",
      "|    M18     | 1            |    M19     | 0            |    M1A     | 0            |\n",
      "|    M1B     | 0            |     M2     | 0            |    M20     | 0            |\n",
      "|    M21     | 0            |    M213    | 0            |    M22     | 0            |\n",
      "|    M22A    | 0            |    M23     | 0            |    M24     | 0            |\n",
      "|    M245    | 0            |    M25     | 0            |    M26     | 0            |\n",
      "|    M29     | 0            |     M3     | 0            |    M30     | 0            |\n",
      "|    M31     | 0            |    M32     | 0            |    M33A    | 0            |\n",
      "|    M33B    | 0            |    M34     | 0            |    M38B    | 0            |\n",
      "|    M39     | 0            |     M4     | 0            |    M40     | 0            |\n",
      "|    M42     | 0            |    M43A    | 0            |    M44     | 0            |\n",
      "|    M46     | 0            |     M5     | 0            |     M6     | 0            |\n",
      "|     M8     | 0            |    M8A     | 0            |     M9     | 0            |\n",
      "|    M9B     | 0            |     N1     | 0            |    N10A    | 0            |\n",
      "|    N11     | 0            |    N13     | 0            |    N14     | 0            |\n",
      "|    N16     | 0            |    N18     | 0            |     N2     | 0            |\n",
      "|    N20     | 0            |    N21     | 0            |    N22     | 0            |\n",
      "|    N23     | 1            |    N25     | 0            |    N26     | 0            |\n",
      "|    N27     | 0            |    N28D    | 0            |    N29     | 0            |\n",
      "|     N3     | 0            |    N30     | 0            |    N31     | 0            |\n",
      "|    N33     | 0            |    N35     | 12           |    N35A    | 0            |\n",
      "|    N36     | 0            |    N37     | 0            |     N4     | 0            |\n",
      "|    N40     | 0            |    N41     | 0            |    N42     | 0            |\n",
      "|    N45     | 0            |    N46     | 0            |     N5     | 0            |\n",
      "|    N50     | 0            |    N64     | 0            |    N71     | 0            |\n",
      "|    N72     | 0            |    N74     | 0            |     N8     | 0            |\n",
      "|    N90     | 0            |     O1     | 3            |    O10     | 0            |\n",
      "|    O12     | 0            |    O16     | 0            |    O16C    | 0            |\n",
      "|    O178    | 0            |    O18     | 0            |    O20     | 0            |\n",
      "|    O21     | 0            |    O22     | 0            |    O22B    | 0            |\n",
      "|    O26     | 0            |    O28     | 0            |    O29     | 0            |\n",
      "|    O30U    | 0            |    O31     | 0            |    O326    | 0            |\n",
      "|    O34     | 1            |    O35     | 0            |    O36     | 0            |\n",
      "|    O38     | 0            |    O39     | 0            |     O4     | 0            |\n",
      "|    O40A    | 0            |    O42     | 0            |    O43     | 0            |\n",
      "|    O44     | 0            |    O48     | 0            |    O49     | 0            |\n",
      "|     O5     | 0            |    O50     | 0            |     O6     | 0            |\n",
      "|     O7     | 0            |     O8     | 0            |     O9     | 0            |\n",
      "|    O94     | 0            |     P1     | 0            |    P10     | 0            |\n",
      "|    P11     | 0            |    P1A     | 0            |    P1C     | 0            |\n",
      "|    P20     | 0            |    P30     | 0            |     P4     | 0            |\n",
      "|     P5     | 0            |     P6     | 3            |     P8     | 0            |\n",
      "|    P8H     | 0            |     P9     | 0            |     Q1     | 0            |\n",
      "|    Q19     | 0            |     Q3     | 0            |     Q6     | 0            |\n",
      "|    Q6G     | 0            |     Q7     | 0            |    Q7A     | 0            |\n",
      "|    Q7G     | 0            |     R1     | 0            |    R10     | 0            |\n",
      "|    R11     | 4            |    R118    | 0            |    R12     | 0            |\n",
      "|    R14     | 0            |    R15     | 0            |    R15A    | 0            |\n",
      "|    R16     | 0            |     R2     | 0            |    R20     | 0            |\n",
      "|    R20A    | 0            |    R22     | 0            |    R23     | 0            |\n",
      "|    R24     | 0            |     R4     | 0            |     R5     | 0            |\n",
      "|    R50     | 0            |    R61A    | 0            |     R7     | 0            |\n",
      "|     R8     | 0            |    R8A     | 0            |     S1     | 0            |\n",
      "|    S12     | 0            |    S13     | 0            |    S15     | 0            |\n",
      "|    S15A    | 0            |    S198    | 0            |     S2     | 0            |\n",
      "|    S20     | 0            |    S23     | 0            |    S24     | 1            |\n",
      "|    S27     | 0            |    S28     | 0            |    S29     | 6            |\n",
      "|     S3     | 0            |    S32     | 0            |    S32A    | 0            |\n",
      "|    S33     | 0            |    S34     | 1            |    S35     | 0            |\n",
      "|    S36     | 0            |    S38     | 0            |    S40     | 0            |\n",
      "|    S41     | 0            |    S42     | 0            |    S42A    | 0            |\n",
      "|    S43     | 0            |    S47A    | 0            |    S57A    | 0            |\n",
      "|    S57B    | 0            |     S8     | 0            |    T10     | 0            |\n",
      "|    T101    | 0            |    T11B    | 0            |    T12     | 0            |\n",
      "|    T121    | 0            |    T14     | 0            |    T18     | 0            |\n",
      "|    T19A    | 0            |    T21     | 0            |    T22     | 0            |\n",
      "|    T24     | 0            |    T25     | 0            |    T26     | 0            |\n",
      "|    T28     | 0            |    T29     | 0            |     T3     | 0            |\n",
      "|    T30     | 0            |    T30A    | 0            |    T31     | 0            |\n",
      "|    T34     | 0            |    T35     | 0            |     T5     | 0            |\n",
      "|     T8     | 0            |     T9     | 0            |    T90     | 0            |\n",
      "|    T92A    | 0            |    T9C     | 0            |     U1     | 0            |\n",
      "|    U10     | 0            |    U13     | 0            |    U15     | 0            |\n",
      "|    U17     | 0            |    U19     | 0            |     U2     | 0            |\n",
      "|    U20     | 0            |    U21     | 0            |    U22     | 0            |\n",
      "|    U23     | 0            |    U23B    | 0            |    U24     | 0            |\n",
      "|    U24A    | 0            |    U25     | 0            |    U26     | 0            |\n",
      "|    U29     | 0            |    U30     | 0            |    U31     | 2            |\n",
      "|    U32     | 0            |    U33     | 4            |    U35     | 0            |\n",
      "|    U36     | 1            |    U38A    | 0            |    U39I    | 0            |\n",
      "|    U40     | 0            |     U5     | 0            |     U7     | 0            |\n",
      "|     U8     | 0            |     U9     | 0            |     V1     | 7            |\n",
      "|    V10     | 0            |    V102    | 0            |    V11     | 0            |\n",
      "|    V12     | 0            |    V13     | 0            |    V15     | 0            |\n",
      "|    V17     | 0            |    V19     | 0            |     V2     | 0            |\n",
      "|    V20     | 0            |    V21     | 0            |    V22     | 0            |\n",
      "|    V23     | 0            |    V23A    | 0            |    V24     | 0            |\n",
      "|    V25     | 0            |    V26     | 0            |    V27     | 0            |\n",
      "|    V28     | 1            |    V28A    | 0            |    V29     | 0            |\n",
      "|    V29A    | 0            |    V30     | 0            |    V31     | 4            |\n",
      "|    V33     | 0            |    V36     | 0            |    V36G    | 0            |\n",
      "|    V39     | 0            |     V4     | 1            |     V5     | 0            |\n",
      "|     V6     | 0            |     V7     | 0            |    W10     | 0            |\n",
      "|    W11     | 0            |    W13     | 0            |    W14     | 0            |\n",
      "|    W14A    | 0            |    W15     | 0            |    W15B    | 0            |\n",
      "|    W16     | 0            |    W17     | 0            |    W17A    | 0            |\n",
      "|    W17C    | 0            |    W18A    | 0            |    W19     | 0            |\n",
      "|     W2     | 0            |    W20     | 0            |    W22     | 0            |\n",
      "|    W24     | 0            |    W25     | 1            |    W2B     | 0            |\n",
      "|     W3     | 0            |     W4     | 0            |    W42     | 0            |\n",
      "|    W51     | 0            |     W8     | 0            |     W9     | 0            |\n",
      "|     X1     | 9            |     X2     | 0            |     X3     | 0            |\n",
      "|     X4     | 0            |     X5     | 0            |     X6     | 0            |\n",
      "|     X8     | 0            |     Y1     | 7            |     Y2     | 0            |\n",
      "|     Y3     | 0            |     Y4     | 0            |     Y5     | 3            |\n",
      "|     Z1     | 4            |    Z11     | 0            |    Z11A    | 0            |\n",
      "|    Z1A     | 0            |     Z2     | 2            |    Z20     | 0            |\n",
      "|    Z2C     | 0            |     Z3     | 0            |    Z3A     | 1            |\n",
      "|     Z4     | 4            |    Z4A     | 0            |     Z5     | 0            |\n",
      "|     Z7     | 0            |     Z9     | 0            |    Aa1     | 5            |\n",
      "|    S19     | 0            |    Aa17    | 0            |    Aa11    | 0            |\n",
      "|    Aa28    | 0            |    Aa27    | 0            |    Aa26    | 0            |\n",
      "|    A32h    | 0            |     G7     | 2            |    U28     | 3            |\n",
      "|    Aa15    | 1            |    Line    | 0            |    S39     | 1            |\n",
      "|    Aa21    | 1            |            |              |            |              |\n",
      "|   total    | 191          |            |              |            |              |\n",
      "[08/22 20:58:21 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=1024, sample_style='choice')]\n",
      "[08/22 20:58:21 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[08/22 20:58:21 d2.data.common]: Serializing 1 elements to byte tensors and concatenating them all ...\n",
      "[08/22 20:58:21 d2.data.common]: Serialized dataset takes 0.01 MiB\n",
      "Evaluating on hieroglyph_test_improved...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating hieroglyph_test_improved: 100%|| 1/1 [00:00<00:00,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/22 20:58:21 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[08/22 20:58:21 d2.evaluation.coco_evaluation]: Saving results to ./output/improved_training_20250822_200344/coco_instances_results.json\n",
      "[08/22 20:58:21 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[08/22 20:58:21 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[08/22 20:58:21 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[08/22 20:58:21 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/22 20:58:22 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 1.49 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.322\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.252\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.232\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.131\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.229\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.233\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.233\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "[08/22 20:58:22 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 21.371 | 32.188 | 25.227 | 23.194 |  nan  |  nan  |\n",
      "[08/22 20:58:22 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
      "[08/22 20:58:22 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| A1         | 35.188 | A121C      | 0.000  | A13        | nan    |\n",
      "| A131A      | nan    | A13A       | nan    | A14        | nan    |\n",
      "| A15        | nan    | A16        | nan    | A169       | nan    |\n",
      "| A17        | nan    | A183       | nan    | A19        | nan    |\n",
      "| A1B        | nan    | A2         | 49.010 | A20        | nan    |\n",
      "| A21        | nan    | A21A       | nan    | A22        | nan    |\n",
      "| A23        | nan    | A239B      | nan    | A23A       | nan    |\n",
      "| A24        | 0.000  | A25        | nan    | A26        | nan    |\n",
      "| A27        | nan    | A28        | nan    | A29        | nan    |\n",
      "| A3         | nan    | A30        | nan    | A317       | nan    |\n",
      "| A32A       | nan    | A35        | nan    | A4         | nan    |\n",
      "| A40        | nan    | A406       | nan    | A42        | nan    |\n",
      "| A43B       | nan    | A44        | nan    | A45A       | nan    |\n",
      "| A47        | nan    | A49        | nan    | A50        | 0.000  |\n",
      "| A51        | nan    | A52        | nan    | A53        | nan    |\n",
      "| A55        | nan    | A73        | nan    | A7A        | nan    |\n",
      "| A8         | nan    | A81        | nan    | A86        | nan    |\n",
      "| A9         | nan    | A90        | nan    | A9C        | nan    |\n",
      "| B1         | 60.000 | B10        | nan    | B12        | nan    |\n",
      "| B17        | nan    | B2         | nan    | B2A        | nan    |\n",
      "| B3         | nan    | B3A        | nan    | B3B        | nan    |\n",
      "| B5         | nan    | B59        | nan    | B5C        | nan    |\n",
      "| C1         | nan    | C10        | nan    | C10A       | nan    |\n",
      "| C11        | nan    | C164       | nan    | C19        | nan    |\n",
      "| C1D        | nan    | C4         | nan    | C9         | nan    |\n",
      "| D1         | 75.545 | D10        | nan    | D12        | nan    |\n",
      "| D13        | nan    | D143       | nan    | D19        | nan    |\n",
      "| D192       | nan    | D198       | nan    | D2         | 0.000  |\n",
      "| D20        | nan    | D200       | nan    | D21        | 20.000 |\n",
      "| D24        | nan    | D245       | nan    | D25        | nan    |\n",
      "| D26        | nan    | D26A       | nan    | D28        | nan    |\n",
      "| D283       | nan    | D284       | nan    | D285       | nan    |\n",
      "| D3         | nan    | D329       | nan    | D33        | nan    |\n",
      "| D33C       | nan    | D34        | nan    | D35        | 20.198 |\n",
      "| D36        | 0.000  | D37        | nan    | D38        | nan    |\n",
      "| D39        | nan    | D3B        | nan    | D4         | 0.000  |\n",
      "| D40        | 70.000 | D41        | nan    | D42        | nan    |\n",
      "| D43        | nan    | D45        | nan    | D46        | 36.172 |\n",
      "| D49        | nan    | D5         | nan    | D50        | nan    |\n",
      "| D51        | nan    | D52        | nan    | D53        | nan    |\n",
      "| D54        | 0.000  | D55        | nan    | D56        | nan    |\n",
      "| D57        | nan    | D57B       | nan    | D58        | 0.000  |\n",
      "| D6         | nan    | D60        | nan    | D63        | nan    |\n",
      "| D7C        | nan    | D9A        | nan    | E1         | nan    |\n",
      "| E10        | nan    | E100       | nan    | E11        | nan    |\n",
      "| E11B       | nan    | E12        | nan    | E128       | nan    |\n",
      "| E13        | nan    | E14        | nan    | E15        | nan    |\n",
      "| E16        | nan    | E164       | nan    | E17        | nan    |\n",
      "| E18        | nan    | E186       | nan    | E1A        | nan    |\n",
      "| E2         | nan    | E23        | nan    | E263       | nan    |\n",
      "| E27        | nan    | E3         | nan    | E31        | nan    |\n",
      "| E32        | nan    | E34        | nan    | E35        | nan    |\n",
      "| E51        | nan    | E7         | nan    | E8         | 80.000 |\n",
      "| E9         | nan    | F1         | nan    | F10        | nan    |\n",
      "| F12        | nan    | F13        | nan    | F16        | nan    |\n",
      "| F17        | nan    | F18        | nan    | F19        | nan    |\n",
      "| F20        | nan    | F21        | nan    | F22        | nan    |\n",
      "| F23        | nan    | F25        | nan    | F25A       | nan    |\n",
      "| F26        | nan    | F27        | nan    | F29        | nan    |\n",
      "| F2B        | nan    | F30        | nan    | F30A       | nan    |\n",
      "| F31        | nan    | F32        | nan    | F33        | nan    |\n",
      "| F34        | nan    | F35        | nan    | F36        | nan    |\n",
      "| F37        | nan    | F37B       | nan    | F37C       | nan    |\n",
      "| F37I       | nan    | F37N       | nan    | F39        | nan    |\n",
      "| F4         | nan    | F40        | nan    | F41        | nan    |\n",
      "| F42        | nan    | F43        | nan    | F44        | nan    |\n",
      "| F45        | nan    | F46        | nan    | F47        | nan    |\n",
      "| F5         | nan    | F50        | nan    | F51        | nan    |\n",
      "| F7         | nan    | F7C        | nan    | F8         | nan    |\n",
      "| F9         | nan    | G1         | 12.418 | G14        | nan    |\n",
      "| G17        | 30.297 | G17A       | nan    | G18        | nan    |\n",
      "| G197       | nan    | G2         | nan    | G21        | nan    |\n",
      "| G21A       | nan    | G21B       | nan    | G21C       | nan    |\n",
      "| G22        | nan    | G23        | nan    | G25        | nan    |\n",
      "| G253       | nan    | G26        | nan    | G26A       | nan    |\n",
      "| G26B       | nan    | G27        | nan    | G28        | nan    |\n",
      "| G29        | 0.000  | G30        | nan    | G30A       | nan    |\n",
      "| G31        | nan    | G32        | nan    | G35        | nan    |\n",
      "| G36        | 0.000  | G37        | nan    | G38        | 0.000  |\n",
      "| G39        | nan    | G4         | nan    | G40        | nan    |\n",
      "| G41        | 0.000  | G43        | nan    | G47        | nan    |\n",
      "| G49C       | nan    | G49E       | nan    | G4A        | nan    |\n",
      "| G5         | nan    | G50        | nan    | G51        | nan    |\n",
      "| G54        | nan    | G6         | nan    | H1         | nan    |\n",
      "| H3         | nan    | H5         | nan    | H6         | nan    |\n",
      "| H8         | nan    | I1         | nan    | I10        | 47.921 |\n",
      "| I12        | nan    | I13        | nan    | I14        | nan    |\n",
      "| I2         | nan    | I24A       | nan    | I3         | nan    |\n",
      "| I5         | nan    | I6         | nan    | I70B       | nan    |\n",
      "| I70D       | nan    | I71A       | nan    | I8         | nan    |\n",
      "| I86C       | nan    | I86D       | nan    | I87        | nan    |\n",
      "| I9         | 0.481  | I9A        | nan    | K1         | nan    |\n",
      "| K2         | nan    | K3         | nan    | K4         | nan    |\n",
      "| K5         | nan    | K7         | nan    | L1         | nan    |\n",
      "| L2         | nan    | L4         | nan    | L5         | nan    |\n",
      "| L7         | nan    | M1         | nan    | M11        | nan    |\n",
      "| M12        | nan    | M13        | nan    | M14        | nan    |\n",
      "| M15        | 70.000 | M16        | nan    | M17        | 44.597 |\n",
      "| M18        | 0.000  | M19        | nan    | M1A        | nan    |\n",
      "| M1B        | nan    | M2         | nan    | M20        | nan    |\n",
      "| M21        | nan    | M213       | nan    | M22        | nan    |\n",
      "| M22A       | nan    | M23        | nan    | M24        | nan    |\n",
      "| M245       | nan    | M25        | nan    | M26        | nan    |\n",
      "| M29        | nan    | M3         | nan    | M30        | nan    |\n",
      "| M31        | nan    | M32        | nan    | M33A       | nan    |\n",
      "| M33B       | nan    | M34        | nan    | M38B       | nan    |\n",
      "| M39        | nan    | M4         | nan    | M40        | nan    |\n",
      "| M42        | nan    | M43A       | nan    | M44        | nan    |\n",
      "| M46        | nan    | M5         | nan    | M6         | nan    |\n",
      "| M8         | nan    | M8A        | nan    | M9         | nan    |\n",
      "| M9B        | nan    | N1         | nan    | N10A       | nan    |\n",
      "| N11        | nan    | N13        | nan    | N14        | nan    |\n",
      "| N16        | nan    | N18        | nan    | N2         | nan    |\n",
      "| N20        | nan    | N21        | nan    | N22        | nan    |\n",
      "| N23        | 0.000  | N25        | nan    | N26        | nan    |\n",
      "| N27        | nan    | N28D       | nan    | N29        | nan    |\n",
      "| N3         | nan    | N30        | nan    | N31        | nan    |\n",
      "| N33        | nan    | N35        | 14.802 | N35A       | nan    |\n",
      "| N36        | nan    | N37        | nan    | N4         | nan    |\n",
      "| N40        | nan    | N41        | nan    | N42        | nan    |\n",
      "| N45        | nan    | N46        | nan    | N5         | nan    |\n",
      "| N50        | nan    | N64        | nan    | N71        | nan    |\n",
      "| N72        | nan    | N74        | nan    | N8         | nan    |\n",
      "| N90        | nan    | O1         | 23.564 | O10        | nan    |\n",
      "| O12        | nan    | O16        | nan    | O16C       | nan    |\n",
      "| O178       | nan    | O18        | nan    | O20        | nan    |\n",
      "| O21        | nan    | O22        | nan    | O22B       | nan    |\n",
      "| O26        | nan    | O28        | nan    | O29        | nan    |\n",
      "| O30U       | nan    | O31        | nan    | O326       | nan    |\n",
      "| O34        | 0.000  | O35        | nan    | O36        | nan    |\n",
      "| O38        | nan    | O39        | nan    | O4         | nan    |\n",
      "| O40A       | nan    | O42        | nan    | O43        | nan    |\n",
      "| O44        | nan    | O48        | nan    | O49        | nan    |\n",
      "| O5         | nan    | O50        | nan    | O6         | nan    |\n",
      "| O7         | nan    | O8         | nan    | O9         | nan    |\n",
      "| O94        | nan    | P1         | nan    | P10        | nan    |\n",
      "| P11        | nan    | P1A        | nan    | P1C        | nan    |\n",
      "| P20        | nan    | P30        | nan    | P4         | nan    |\n",
      "| P5         | nan    | P6         | 0.000  | P8         | nan    |\n",
      "| P8H        | nan    | P9         | nan    | Q1         | nan    |\n",
      "| Q19        | nan    | Q3         | nan    | Q6         | nan    |\n",
      "| Q6G        | nan    | Q7         | nan    | Q7A        | nan    |\n",
      "| Q7G        | nan    | R1         | nan    | R10        | nan    |\n",
      "| R11        | 61.584 | R118       | nan    | R12        | nan    |\n",
      "| R14        | nan    | R15        | nan    | R15A       | nan    |\n",
      "| R16        | nan    | R2         | nan    | R20        | nan    |\n",
      "| R20A       | nan    | R22        | nan    | R23        | nan    |\n",
      "| R24        | nan    | R4         | nan    | R5         | nan    |\n",
      "| R50        | nan    | R61A       | nan    | R7         | nan    |\n",
      "| R8         | nan    | R8A        | nan    | S1         | nan    |\n",
      "| S12        | nan    | S13        | nan    | S15        | nan    |\n",
      "| S15A       | nan    | S198       | nan    | S2         | nan    |\n",
      "| S20        | nan    | S23        | nan    | S24        | 0.000  |\n",
      "| S27        | nan    | S28        | nan    | S29        | 28.614 |\n",
      "| S3         | nan    | S32        | nan    | S32A       | nan    |\n",
      "| S33        | nan    | S34        | 0.000  | S35        | nan    |\n",
      "| S36        | nan    | S38        | nan    | S40        | nan    |\n",
      "| S41        | nan    | S42        | nan    | S42A       | nan    |\n",
      "| S43        | nan    | S47A       | nan    | S57A       | nan    |\n",
      "| S57B       | nan    | S8         | nan    | T10        | nan    |\n",
      "| T101       | nan    | T11B       | nan    | T12        | nan    |\n",
      "| T121       | nan    | T14        | nan    | T18        | nan    |\n",
      "| T19A       | nan    | T21        | nan    | T22        | nan    |\n",
      "| T24        | nan    | T25        | nan    | T26        | nan    |\n",
      "| T28        | nan    | T29        | nan    | T3         | nan    |\n",
      "| T30        | nan    | T30A       | nan    | T31        | nan    |\n",
      "| T34        | nan    | T35        | nan    | T5         | nan    |\n",
      "| T8         | nan    | T9         | nan    | T90        | nan    |\n",
      "| T92A       | nan    | T9C        | nan    | U1         | nan    |\n",
      "| U10        | nan    | U13        | nan    | U15        | nan    |\n",
      "| U17        | nan    | U19        | nan    | U2         | nan    |\n",
      "| U20        | nan    | U21        | nan    | U22        | nan    |\n",
      "| U23        | nan    | U23B       | nan    | U24        | nan    |\n",
      "| U24A       | nan    | U25        | nan    | U26        | nan    |\n",
      "| U29        | nan    | U30        | nan    | U31        | 40.396 |\n",
      "| U32        | nan    | U33        | 48.647 | U35        | nan    |\n",
      "| U36        | 0.000  | U38A       | nan    | U39I       | nan    |\n",
      "| U40        | nan    | U5         | nan    | U7         | nan    |\n",
      "| U8         | nan    | U9         | nan    | V1         | 17.970 |\n",
      "| V10        | nan    | V102       | nan    | V11        | nan    |\n",
      "| V12        | nan    | V13        | nan    | V15        | nan    |\n",
      "| V17        | nan    | V19        | nan    | V2         | nan    |\n",
      "| V20        | nan    | V21        | nan    | V22        | nan    |\n",
      "| V23        | nan    | V23A       | nan    | V24        | nan    |\n",
      "| V25        | nan    | V26        | nan    | V27        | nan    |\n",
      "| V28        | 0.000  | V28A       | nan    | V29        | nan    |\n",
      "| V29A       | nan    | V30        | nan    | V31        | 0.000  |\n",
      "| V33        | nan    | V36        | nan    | V36G       | nan    |\n",
      "| V39        | nan    | V4         | 0.000  | V5         | nan    |\n",
      "| V6         | nan    | V7         | nan    | W10        | nan    |\n",
      "| W11        | nan    | W13        | nan    | W14        | nan    |\n",
      "| W14A       | nan    | W15        | nan    | W15B       | nan    |\n",
      "| W16        | nan    | W17        | nan    | W17A       | nan    |\n",
      "| W17C       | nan    | W18A       | nan    | W19        | nan    |\n",
      "| W2         | nan    | W20        | nan    | W22        | nan    |\n",
      "| W24        | nan    | W25        | 45.000 | W2B        | nan    |\n",
      "| W3         | nan    | W4         | nan    | W42        | nan    |\n",
      "| W51        | nan    | W8         | nan    | W9         | nan    |\n",
      "| X1         | 14.587 | X2         | nan    | X3         | nan    |\n",
      "| X4         | nan    | X5         | nan    | X6         | nan    |\n",
      "| X8         | nan    | Y1         | 27.847 | Y2         | nan    |\n",
      "| Y3         | nan    | Y4         | nan    | Y5         | 0.000  |\n",
      "| Z1         | 0.000  | Z11        | nan    | Z11A       | nan    |\n",
      "| Z1A        | nan    | Z2         | 0.000  | Z20        | nan    |\n",
      "| Z2C        | nan    | Z3         | nan    | Z3A        | 80.000 |\n",
      "| Z4         | 18.020 | Z4A        | nan    | Z5         | nan    |\n",
      "| Z7         | nan    | Z9         | nan    | Aa1        | 39.307 |\n",
      "| S19        | nan    | Aa17       | nan    | Aa11       | nan    |\n",
      "| Aa28       | nan    | Aa27       | nan    | Aa26       | nan    |\n",
      "| A32h       | nan    | G7         | 40.396 | U28        | 26.931 |\n",
      "| Aa15       | 60.000 | Line       | nan    | S39        | 0.000  |\n",
      "| Aa21       | 0.000  |            |        |            |        |\n",
      "\n",
      "IMPROVED MODEL RESULTS:\n",
      "   mAP: 21.371\n",
      "   mAP@50: 32.188\n",
      "   mAP@75: 25.227\n",
      "\n",
      "IMPROVEMENT ANALYSIS:\n",
      "   Previous mAP: 57.2%\n",
      "   New mAP: 21.4%\n",
      "   Improvement: -35.8%\n",
      "   mAP may be lower, but let's check detection count...\n"
     ]
    }
   ],
   "source": [
    "print(\"EVALUATING IMPROVED MODEL\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Use the trained model weights - load the latest checkpoint\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3  # Critical fix!\n",
    "\n",
    "# Build the model directly using the configuration from the training\n",
    "model = trainer.build_model(cfg) # Reuse the build_model from our trainer\n",
    "\n",
    "# Find and load the latest checkpoint\n",
    "checkpointer = DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR)\n",
    "latest_checkpoint = checkpointer.get_checkpoint_file()\n",
    "\n",
    "if latest_checkpoint:\n",
    "    print(f\"Loading latest checkpoint: {latest_checkpoint}\")\n",
    "    checkpointer.load(latest_checkpoint)\n",
    "    print(\"Checkpoint loaded.\")\n",
    "else:\n",
    "    print(f\"No checkpoint found in {cfg.OUTPUT_DIR}. Cannot evaluate.\")\n",
    "    # Exit or handle the case where no model is available\n",
    "    raise FileNotFoundError(f\"No checkpoint found in {cfg.OUTPUT_DIR}. Cannot evaluate.\")\n",
    "\n",
    "\n",
    "# Ensure model is in evaluation mode and on the correct device\n",
    "model.eval()\n",
    "model.to(cfg.MODEL.DEVICE)\n",
    "\n",
    "print(f\"Using confidence threshold: {cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST}\")\n",
    "\n",
    "# Evaluate on test set manually for explicit device handling\n",
    "evaluator = COCOEvaluator(\"hieroglyph_test_improved\", output_dir=cfg.OUTPUT_DIR)\n",
    "data_loader = build_detection_test_loader(cfg, \"hieroglyph_test_improved\")\n",
    "\n",
    "print(f\"Evaluating on hieroglyph_test_improved...\")\n",
    "evaluator.reset()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs in tqdm(data_loader, desc=f\"Evaluating hieroglyph_test_improved\"):\n",
    "        # Explicitly move inputs to the model's device\n",
    "        # Reuse the helper method from trainer, ensure trainer object exists or define helper locally\n",
    "        # Assuming trainer object is available from the previous cell run\n",
    "        inputs = trainer._move_inputs_to_device(inputs, cfg.MODEL.DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        evaluator.process(inputs, outputs)\n",
    "\n",
    "results = evaluator.evaluate()\n",
    "\n",
    "\n",
    "print(\"\\nIMPROVED MODEL RESULTS:\")\n",
    "print(f\"mAP: {results['bbox']['AP']:.3f}\")\n",
    "print(f\"mAP@50: {results['bbox']['AP50']:.3f}\")\n",
    "print(f\"mAP@75: {results['bbox']['AP75']:.3f}\")\n",
    "\n",
    "# Compare with previous results\n",
    "previous_map = 57.2  # Your previous best\n",
    "improvement = results['bbox']['AP'] - previous_map\n",
    "\n",
    "print(f\"\\nIMPROVEMENT ANALYSIS:\")\n",
    "print(f\"Previous mAP: {previous_map:.1f}%\")\n",
    "print(f\"New mAP: {results['bbox']['AP']:.1f}%\")\n",
    "print(f\"Improvement: {improvement:+.1f}%\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(\"IMPROVEMENT ACHIEVED!\")\n",
    "else:\n",
    "    print(\"mAP may be lower, but let's check detection count...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "detection_test_section"
   },
   "source": [
    "## Detection Count Test\n",
    "\n",
    "The most important test - how many hieroglyphs does the improved model detect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1239,
     "status": "ok",
     "timestamp": 1755896615468,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "test_detection_count",
    "outputId": "76f9c2f2-4f19-4adb-fe43-c2bb527d4d5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING DETECTION COUNT - THE CRITICAL METRIC\n",
      "==================================================\n",
      "[08/22 21:03:34 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from ./output/improved_training_20250822_200344/model_final.pth ...\n",
      "DETECTION COUNT COMPARISON:\n",
      "Image                     GT Count   Previous   New        Improvement \n",
      "----------------------------------------------------------------------\n",
      "patch_0000.png            266        14         164        +150 (+1071.4%)\n",
      "   Detected classes: D1, Z3A, E8, Z4, V31, V1, D40, U31, N35, I9...\n",
      "patch_0000.png            266        19         99         +80 (+421.1%)\n",
      "   Detected classes: Z1, M6, Z4, V31, V28, V1, D40, N35, I9, U33...\n",
      "patch_0001.png            200        15         181        +166 (+1106.7%)\n",
      "   Detected classes: D1, S34, M6, E8, Z4, D37, V30, V1, D40, U31...\n",
      "\n",
      "OVERALL DETECTION IMPROVEMENT:\n",
      "   Total additional detections: +396\n",
      "   Average improvement per image: +132.0\n",
      "   MAJOR IMPROVEMENT! Successfully detecting many more hieroglyphs!\n",
      "\n",
      "Remember: Ground truth has 266+ hieroglyphs per image!\n",
      "   Target: Detect at least 50% (130+) hieroglyphs per image\n"
     ]
    }
   ],
   "source": [
    "print(\"TESTING DETECTION COUNT - THE CRITICAL METRIC\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Configure the predictor\n",
    "# Ensure cfg object is available from previous cells\n",
    "# Use the trained model weights (latest checkpoint or model_final.pth)\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, DetectionCheckpointer(trainer.model, save_dir=cfg.OUTPUT_DIR).get_checkpoint_file().split('/')[-1]) # Load the latest checkpoint\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3 # Use the lower confidence threshold\n",
    "\n",
    "# Create the predictor\n",
    "# DefaultPredictor handles loading weights from cfg.MODEL.WEIGHTS and setting up the model\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "def count_detections_in_image(image_path, confidence_threshold=0.3):\n",
    "    \"\"\"Count detections in a specific image using the configured predictor\"\"\"\n",
    "    import cv2\n",
    "    # class_names needs to be available in the scope of this function if used\n",
    "\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Warning: Could not read image {image_path}\")\n",
    "        return 0, []\n",
    "\n",
    "    # Run prediction using the globally defined predictor\n",
    "    outputs = predictor(image)\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "\n",
    "    # Filter by confidence (already handled by predictor's SCORE_THRESH_TEST, but double-check)\n",
    "    # It's better to rely on the predictor's threshold set in cfg\n",
    "    # high_conf_mask = instances.scores > confidence_threshold\n",
    "    # filtered_instances = instances[high_conf_mask]\n",
    "\n",
    "    # Get class counts - Filtered by predictor's SCORE_THRESH_TEST and NMS\n",
    "    filtered_instances = instances # Use instances directly as predictor applied threshold and NMS\n",
    "\n",
    "    # Get class names - Ensure class_names list is available\n",
    "    # Assuming class_names is defined in a previous cell and is globally accessible\n",
    "    if 'class_names' in globals() and len(filtered_instances) > 0:\n",
    "        classes = filtered_instances.pred_classes.numpy()\n",
    "        detected_classes = [class_names[cls_idx] for cls_idx in classes if cls_idx < len(class_names)]\n",
    "    else:\n",
    "         detected_classes = []\n",
    "         if len(filtered_instances) > 0 and 'class_names' not in globals():\n",
    "             print(\"Warning: class_names not found, cannot list detected classes.\")\n",
    "\n",
    "\n",
    "    return len(filtered_instances), detected_classes\n",
    "\n",
    "# Test on the critical images\n",
    "test_images = [\n",
    "    \"hieroglyphs_dataset/test/images/patch_0000.png\",\n",
    "    \"hieroglyphs_dataset/val/images/patch_0000.png\",\n",
    "    \"hieroglyphs_dataset/val/images/patch_0001.png\"\n",
    "]\n",
    "\n",
    "ground_truth_counts = [266, 266, 200]  # From our previous analysis\n",
    "previous_detections = [14, 19, 15]  # Previous model results\n",
    "\n",
    "print(\"DETECTION COUNT COMPARISON:\")\n",
    "print(f\"{'Image':<25} {'GT Count':<10} {'Previous':<10} {'New':<10} {'Improvement':<12}\")\n",
    "print(\"-\"* 70)\n",
    "\n",
    "total_improvement = 0\n",
    "for i, image_path in enumerate(test_images):\n",
    "    # Ensure os module is imported\n",
    "    import os\n",
    "    if os.path.exists(image_path):\n",
    "        # Pass confidence_threshold to function, though predictor uses cfg's\n",
    "        new_count, detected_classes = count_detections_in_image(image_path, confidence_threshold=0.3)\n",
    "        improvement = new_count - previous_detections[i]\n",
    "        total_improvement += improvement\n",
    "\n",
    "        image_name = os.path.basename(image_path)\n",
    "        # Avoid division by zero if previous_detections[i] is 0\n",
    "        improvement_percentage = (improvement / previous_detections[i] * 100) if previous_detections[i] != 0 else 0\n",
    "        print(f\"{image_name:<25} {ground_truth_counts[i]:<10} {previous_detections[i]:<10} {new_count:<10} {improvement:+d} ({improvement_percentage:+.1f}%)\")\n",
    "\n",
    "        # Show some detected classes\n",
    "        if detected_classes:\n",
    "            unique_classes = list(set(detected_classes))\n",
    "            print(f\"Detected classes: {', '.join(unique_classes[:10])}{'...' if len(unique_classes) > 10 else ''}\")\n",
    "\n",
    "print(f\"\\nOVERALL DETECTION IMPROVEMENT:\")\n",
    "print(f\"Total additional detections: +{total_improvement}\")\n",
    "print(f\"Average improvement per image: +{total_improvement/len(test_images):.1f}\")\n",
    "\n",
    "if total_improvement > 50:\n",
    "    print(\"MAJOR IMPROVEMENT! Successfully detecting many more hieroglyphs!\")\n",
    "elif total_improvement > 20:\n",
    "    print(\"GOOD IMPROVEMENT! Significant increase in detections.\")\n",
    "elif total_improvement > 0:\n",
    "    print(\"MODEST IMPROVEMENT! Some increase in detections.\")\n",
    "else:\n",
    "    print(\"No improvement in detection count. May need further adjustments.\")\n",
    "\n",
    "print(f\"\\nRemember: Ground truth has 266+ hieroglyphs per image!\")\n",
    "print(f\"Target: Detect at least 50% (130+) hieroglyphs per image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps_section"
   },
   "source": [
    "## Next Steps for Further Improvement\n",
    "\n",
    "Based on the results, here are recommended next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 75,
     "status": "ok",
     "timestamp": 1755896625125,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "save_final_results",
    "outputId": "a1dd2978-52c1-4c74-def9-a8673980e815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEXT STEPS FOR CONTINUED IMPROVEMENT:\n",
      "==================================================\n",
      "1. If detection count improved significantly:\n",
      "   - Continue with current approach\n",
      "   - Consider even lower threshold (0.2 or 0.25)\n",
      "   - Fine-tune class weights for specific missed classes\n",
      "\n",
      "2. If detection count improved moderately:\n",
      "   - Implement copy-paste augmentation for rare classes\n",
      "   - Use progressive resizing during training\n",
      "   - Consider ensemble methods\n",
      "\n",
      "3. If minimal improvement:\n",
      "   - Check if ground truth annotations are correct\n",
      "   - Consider different model architecture (RetinaNet, FCOS)\n",
      "   - Implement multi-scale training and testing\n",
      "\n",
      "4. Always recommended:\n",
      "   - Run detailed error analysis on new results\n",
      "   - Visualize detections vs ground truth\n",
      "   - Monitor training curves for overfitting\n",
      "\n",
      "Training complete! Results saved to: ./output/improved_training_20250822_200344\n",
      "Best model: ./output/improved_training_20250822_200344/model_final.pth\n"
     ]
    }
   ],
   "source": [
    "# Save training results and recommendations\n",
    "training_results = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model_path\": cfg.OUTPUT_DIR,\n",
    "    \"improvements_made\": [\n",
    "        \"Lowered confidence threshold from 0.5 to 0.3\",\n",
    "        \"Implemented Focal Loss for hard examples\",\n",
    "        \"Enhanced data augmentation\",\n",
    "        \"Longer training with early stopping\",\n",
    "        \"Increased max detections per image to 300\"\n",
    "    ],\n",
    "    \"configuration\": {\n",
    "        \"confidence_threshold\": cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST,\n",
    "        \"max_detections\": cfg.TEST.DETECTIONS_PER_IMAGE,\n",
    "        \"training_iterations\": cfg.SOLVER.MAX_ITER,\n",
    "        \"learning_rate\": cfg.SOLVER.BASE_LR,\n",
    "        \"batch_size\": cfg.SOLVER.IMS_PER_BATCH\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results\n",
    "with open(os.path.join(cfg.OUTPUT_DIR, \"training_improvements.json\"), \"w\") as f:\n",
    "    json.dump(training_results, f, indent=2)\n",
    "\n",
    "print(\"NEXT STEPS FOR CONTINUED IMPROVEMENT:\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. If detection count improved significantly:\")\n",
    "print(\"- Continue with current approach\")\n",
    "print(\"- Consider even lower threshold (0.2 or 0.25)\")\n",
    "print(\"- Fine-tune class weights for specific missed classes\")\n",
    "print(\"\\n2. If detection count improved moderately:\")\n",
    "print(\"- Implement copy-paste augmentation for rare classes\")\n",
    "print(\"- Use progressive resizing during training\")\n",
    "print(\"- Consider ensemble methods\")\n",
    "print(\"\\n3. If minimal improvement:\")\n",
    "print(\"- Check if ground truth annotations are correct\")\n",
    "print(\"- Consider different model architecture (RetinaNet, FCOS)\")\n",
    "print(\"- Implement multi-scale training and testing\")\n",
    "print(\"\\n4. Always recommended:\")\n",
    "print(\"- Run detailed error analysis on new results\")\n",
    "print(\"- Visualize detections vs ground truth\")\n",
    "print(\"- Monitor training curves for overfitting\")\n",
    "\n",
    "print(f\"\\nTraining complete! Results saved to: {cfg.OUTPUT_DIR}\")\n",
    "print(f\"Best model: {os.path.join(cfg.OUTPUT_DIR, 'model_final.pth')}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
